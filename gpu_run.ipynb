{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make *tf.slim* to work with *tf.data* in *parallel*.\n",
    "### STEPS\n",
    "    * create data pipeline\n",
    "    * load models(Inception V4, Vgg19 and Dense121)\n",
    "    * load ckpt(only weights)\n",
    "    * make it into parallel framework for gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from datasets.utils import get_imgdir_label, label_to_index, parser, preprocessors\n",
    "from models import nets_factory\n",
    "\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "import csv, os, functools, itertools, collections\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config\n",
    "lr = 0.001 #learning_rate\n",
    "batch_size = 32 # batch_size \n",
    "weight_decay = 0.0 # regularizer constant \n",
    "model_name = [\"densenet121\",\"inception_v4\",\"vgg19\"][0] # model selection \n",
    "variable_to_exclude = ['InceptionV4/AuxLogits','InceptionV4/Logits']\n",
    "num_classes = 7 # number of classes\n",
    "weights_loc = \"./weights/\" # weights locations \n",
    "pretrained_dir = glob(weights_loc + \"pretrained/\" + \"*{}*.ckpt\".format(model_name)) #pretrained weights directory\n",
    "trained_dir =  glob(weights_loc + \"trained/\" + \"*{}*.ckpt\".format(model_name))  #post trained weights save location\n",
    "\n",
    "# data config\n",
    "dataset_dir = \"/home/jacob/intel_project_temp/datasets/ISIC_2018/\" # direct to your dataset location\n",
    "class_list = [\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"] # total of 7 classes in the dataset \n",
    "class_dict = {c:i for i,c in enumerate(class_list)} # to map str::class -> int::class for train/test\n",
    "class_frequency = {c:None for c in class_list} # prevenlance of each class in the dataset. Will used to adjust loss.\n",
    "ratio = 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jacob/.virtualenvs/tensorflow-gpu-1.13/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jacob/.virtualenvs/tensorflow-gpu-1.13/lib/python3.5/site-packages/tensorflow/python/ops/image_ops_impl.py:618: to_double (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/jacob/.virtualenvs/tensorflow-gpu-1.13/lib/python3.5/site-packages/tensorflow/python/ops/image_ops_impl.py:619: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 1. create datapipeline\n",
    "tf.reset_default_graph()\n",
    "img_lab = get_imgdir_label(dataset_dir) # get list of tuples [(img_dir_i, str::label_i) for i in range(len(dataset))]\n",
    "img_lab = label_to_index(img_lab, class_dict) # get list of tuples [(img_dir_i, int::label_i) for i in range(len(dataset))]\n",
    "\n",
    "img_lab_train = img_lab[:int(len(img_lab) * ratio)] # train set: 80% of img_lab \n",
    "img_lab_test = list(set(img_lab) - set(img_lab_train)) # test set 20% of img_lab\n",
    "\n",
    "imgs_train, labels_train = zip(*img_lab_train) # transpose train_data \n",
    "imgs_test, labels_test = zip(*img_lab_test) # transpose test_data\n",
    "\n",
    "# establish data_pipeline for trainset \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((list(imgs_train),list(labels_train))) \n",
    "dataset_train = dataset_train.\\\n",
    "           map(parser).\\\n",
    "           batch(32).\\\n",
    "           repeat(32)\n",
    "\n",
    "# establish data_pipeline for testset\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((list(imgs_test),list(labels_test)))\n",
    "dataset_test = dataset_test.\\\n",
    "            map(parser).\\\n",
    "            batch(batch_size).\\\n",
    "            repeat(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "network_fn = nets_factory.get_network_fn(\n",
    "    name = model_name,\n",
    "    weight_decay = weight_decay,\n",
    "    num_classes = num_classes,\n",
    "    is_training = True)\n",
    "\n",
    "train_generator = dataset_train.make_one_shot_iterator()\n",
    "x , y = train_generator.get_next()\n",
    "logits, end_points = network_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusions = [scope.strip()\n",
    "              for scope in variable_to_exclude]\n",
    "\n",
    "# TODO(sguada) variables.filter_variables()\n",
    "variables_to_restore = []\n",
    "for var in slim.get_model_variables():\n",
    "    for exclusion in exclusions:\n",
    "        if var.op.name.startswith(exclusion):\n",
    "            break\n",
    "    else:\n",
    "        variables_to_restore.append(var)\n",
    "\n",
    "        \n",
    "weights_loader = slim.assign_from_checkpoint_fn(\n",
    "    './weights/pretrained/inception_v4.ckpt',\n",
    "    variables_to_restore,\n",
    "    ignore_missing_vars=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    weights_loader(sess)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it into parallel framework for gpu\n",
    "def training_model(input_fn):\n",
    "    inputs = input_fn()\n",
    "    image = inputs[0]\n",
    "    label = tf.cast(inputs[1], tf.int32)\n",
    "    logits = model.forward(image)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss \n",
    "\n",
    "def parallel_training(model_fn, dataset):\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    def input_fn():\n",
    "        with tf.device(None):\n",
    "            # remove any device specifications for the input data\n",
    "            return iterator.get_next()\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1E-3)\n",
    "    \n",
    "    update_op, loss = create_parallel_optimization(model_fn,\n",
    "                                                   input_fn,\n",
    "                                                   optimizer)\n",
    "\n",
    "    do_training(update_op, loss)\n",
    "    \n",
    "\n",
    "\n",
    "def assign_to_device(device, ps_device):\n",
    "    \"\"\"Returns a function to place variables on the ps_device.\n",
    "\n",
    "    Args:\n",
    "        device: Device for everything but variables\n",
    "        ps_device: Device to put the variables on. Example values are /GPU:0 and /CPU:0.\n",
    "\n",
    "    If ps_device is not set then the variables will be placed on the default device.\n",
    "    The best device for shared varibles depends on the platform as well as the\n",
    "    model. Start with CPU:0 and then test GPU:0 to see if there is an\n",
    "    improvement.\n",
    "    \"\"\"\n",
    "    def _assign(op):\n",
    "        \n",
    "        PS_OPS = [\n",
    "        'Variable', 'VariableV2', 'AutoReloadVariable', 'MutableHashTable',\n",
    "        'MutableHashTableOfTensors', 'MutableDenseHashTable']\n",
    "        \n",
    "        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n",
    "        \n",
    "        if node_def.op in PS_OPS:\n",
    "            return ps_device\n",
    "        else:\n",
    "            return device\n",
    "    return _assign    \n",
    "    \n",
    "def do_training(update_op, loss):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        try:\n",
    "            step = 0\n",
    "            while True:\n",
    "                _, loss_value = sess.run((update_op, loss))\n",
    "                if step % 10 == 0:\n",
    "                    print('Step {} with loss {}'.format(step, loss_value))\n",
    "                step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            # we're through the dataset\n",
    "            pass\n",
    "    print('Final loss: {}'.format(loss_value))\n",
    "    \n",
    "def create_parallel_optimization(model_fn, input_fn, optimizer, controller=\"/cpu:0\"):\n",
    "    # This function is defined below; it returns a list of device ids like\n",
    "    # `['/gpu:0', '/gpu:1']`\n",
    "    devices = get_available_gpus()\n",
    "    #devices = ['/gpu:0', '/gpu:1']\n",
    "        \n",
    "    # This list keeps track of the gradients per tower and the losses\n",
    "    tower_grads = []\n",
    "    losses = []\n",
    "    \n",
    "    # Get the current variable scope so we can reuse all variables we need once we get\n",
    "    # to the second iteration of the loop below\n",
    "    with tf.variable_scope(tf.get_variable_scope()) as outer_scope:\n",
    "        for i, id in enumerate(devices):\n",
    "            name = 'tower_{}'.format(i)\n",
    "            # Use the assign_to_device function to ensure that variables are created on the\n",
    "            # controller.\n",
    "            with tf.device(assign_to_device(id, controller)), tf.name_scope(name):\n",
    "                \n",
    "                # Compute loss and gradients, but don't apply them yet\n",
    "                loss = model_fn(input_fn)\n",
    "                \n",
    "                with tf.name_scope(\"compute_gradients\"):\n",
    "                    # `compute_gradients` returns a list of (gradient, variable) pairs\n",
    "                    grads = optimizer.compute_gradients(loss)\n",
    "                    tower_grads.append(grads)\n",
    "                    \n",
    "                losses.append(loss)\n",
    "            \n",
    "            # After the first iteration, we want to reuse the variables.\n",
    "            outer_scope.reuse_variables()\n",
    "                \n",
    "    # Apply the gradients on the controlling device\n",
    "    with tf.name_scope(\"apply_gradients\"), tf.device(controller):\n",
    "        # Note that what we are doing here mathematically is equivalent to returning the\n",
    "        # average loss over the towers and compute the gradients relative to that.\n",
    "        # Unfortunately, this would place all gradient-computations on one device, which is\n",
    "        # why we had to compute the gradients above per tower and need to average them here.\n",
    "        \n",
    "        # This function is defined below; it takes the list of (gradient, variable) lists\n",
    "        # and turns it into a single (gradient, variables) list.\n",
    "        gradients = average_gradients(tower_grads)\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        apply_gradient_op = optimizer.apply_gradients(gradients, global_step)\n",
    "        avg_loss = tf.reduce_mean(losses)\n",
    "\n",
    "    return apply_gradient_op, avg_loss    \n",
    "\n",
    "# Source:\n",
    "# https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "        Returns a list of the identifiers of all visible GPUs.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "\n",
    "# Source:\n",
    "# https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L101\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            if g is not None:\n",
    "                expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(grads, 0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu-1.13",
   "language": "python",
   "name": "tensorflow-gpu-1.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
