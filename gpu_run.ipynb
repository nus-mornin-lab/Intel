{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make *tf.slim* to work with *tf.data* in *parallel*.\n",
    "\n",
    "### working environment \n",
    "    * tensorlow 1.13.1\n",
    "    * python 3.6 \n",
    "    \n",
    "    \n",
    "### dataset \n",
    "    * skin cancer image\n",
    "    \n",
    "### models \n",
    "    * resnet v4, vgg 19, densenet \n",
    "     \n",
    "### STEPS\n",
    "    * create data pipeline\n",
    "    * load models(Inception V4, Vgg19 and Dense121)\n",
    "    * load ckpt(only weights)\n",
    "    * make it into parallel framework for gpu\n",
    "        * get losses of each tower\n",
    "        * get grads of each tower\n",
    "        * get average of grads\n",
    "        * update grads with whatever optimizer you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import csv, os, functools, itertools, collections, six\n",
    "\n",
    "from glob import glob\n",
    "from datasets.utils import get_imgdir_label, label_to_index, parser, preprocessors\n",
    "from models import nets_factory\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config\n",
    "lr = 0.001 #learning_rate\n",
    "batch_size = 32 # batch_size \n",
    "weight_decay = 0.0 # regularizer constant \n",
    "model_name = [\"densenet121\",\"inception_v4\",\"vgg19\"][1] # model selection \n",
    "variable_to_exclude = ['InceptionV4/AuxLogits','InceptionV4/Logits']\n",
    "num_classes = 7 # number of classes\n",
    "weights_loc = \"./weights/\" # weights locations \n",
    "pretrained_dir = glob(weights_loc + \"pretrained/\" + \"*{}*.ckpt\".format(model_name))[0] #pretrained weights directory\n",
    "#trained_dir =  glob(weights_loc + \"trained/\" + \"*{}*.ckpt\".format(model_name))[0]  #post trained weights save location\n",
    "\n",
    "# data config\n",
    "dataset_dir = \"/home/jacob/intel_project_temp/datasets/ISIC_2018/\" # direct to your dataset location\n",
    "class_list = [\"MEL\",\"NV\",\"BCC\",\"AKIEC\",\"BKL\",\"DF\",\"VASC\"] # total of 7 classes in the dataset \n",
    "class_dict = {c:i for i,c in enumerate(class_list)} # to map str::class -> int::class for train/test\n",
    "class_frequency = {c:None for c in class_list} # prevenlance of each class in the dataset. Will used to adjust loss.\n",
    "ratio = 0.8\n",
    "img_size = [224,224]\n",
    "channel_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create datapipeline\n",
    "img_lab = get_imgdir_label(dataset_dir) # get list of tuples [(img_dir_i, str::label_i) for i in range(len(dataset))]\n",
    "img_lab = label_to_index(img_lab, class_dict) # get list of tuples [(img_dir_i, int::label_i) for i in range(len(dataset))]\n",
    "\n",
    "img_lab_train = img_lab[:int(len(img_lab) * ratio)] # train set: 80% of img_lab \n",
    "img_lab_test = list(set(img_lab) - set(img_lab_train)) # test set 20% of img_lab\n",
    "\n",
    "imgs_train, labels_train = zip(*img_lab_train) # transpose train_data \n",
    "imgs_test, labels_test = zip(*img_lab_test) # transpose test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model_fn,inputs):\n",
    "    \n",
    "    images = inputs[0]\n",
    "    labels = tf.cast(inputs[1], tf.int32)\n",
    "    \n",
    "    logits,_ = model_fn(images)\n",
    "     \n",
    "    return logits, labels \n",
    "\n",
    "def forward_backward(model_fn,inputs):\n",
    "    \n",
    "    logits, labels = forward(model_fn, inputs)\n",
    "    \n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        \n",
    "    return tf.reduce_mean(loss), logits, labels\n",
    "\n",
    "def start_training(sess, loss, train_op, acc_train, acc_test):\n",
    "    # inits\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    for i in range(10):\n",
    "        for i in range(200):\n",
    "            loss_realized, _ = sess.run([loss, train_op])\n",
    "            if i % 20 == 0:\n",
    "                print(\"iteration:{}\".format(i))\n",
    "                print(loss_realized)\n",
    "\n",
    "        acc_realized_cum = []\n",
    "        for i in range(100):\n",
    "            acc_realized = sess.run(acc_test)\n",
    "            acc_realized_cum.append(acc_realized)\n",
    "        print(\"test_accuracy;{}\".format(np.sum(acc_realized_cum)))\n",
    "        \n",
    "def serial_train(model_train_fn, model_test_fn, input_train_fn, input_test_fn):\n",
    "    \n",
    "    geneartor_train = input_train_fn.make_one_shot_iterator()\n",
    "    genearator_test = input_test_fn.make_one_shot_iterator()\n",
    "    \n",
    "    loss, logits_train, labels_train = forward_backward(model_train_fn, geneartor_train.get_next())\n",
    "    logits_test, labels_test = forward(model_test_fn, genearator_test.get_next())\n",
    "    \n",
    "    # cross entropy loss \n",
    "    equality_train = tf.equal(labels_train, tf.cast(tf.argmax(logits_train, 1), tf.int32))\n",
    "    acc_train = tf.reduce_mean(tf.cast(equality_train, tf.float32))\n",
    "\n",
    "    equality_test = tf.equal(labels_test, tf.cast(tf.argmax(logits_test, 1), tf.int32))\n",
    "    acc_test = tf.reduce_mean(tf.cast(equality_test, tf.float32))\n",
    "    \n",
    "    # optimize\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    train_op = tf.train.AdadeltaOptimizer(lr).minimize(loss, global_step)  \n",
    "    \n",
    "    # list of parameters to load\n",
    "    get_model_var_list = lambda : slim.get_model_variables()\n",
    "    is_in_exclusion = lambda exclusions, x : [x for exclusion in exclusions if x.op.name.startswith(exclusion) ]\n",
    "    is_in_model_exclusion = functools.partial(is_in_exclusion, variable_to_exclude)\n",
    "    variable_to_include = list(set(get_model_var_list()) - set(filter(is_in_model_exclusion, get_model_var_list())))\n",
    "    \n",
    "    weights_loader = slim.assign_from_checkpoint_fn(\n",
    "        pretrained_dir,\n",
    "        variable_to_include,\n",
    "        ignore_missing_vars=True)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        weights_loader(sess)\n",
    "        start_training(sess, loss, train_op, acc_train, acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# establish data_pipeline for trainset \n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((list(imgs_train),list(labels_train))) \n",
    "dataset_train = dataset_train.\\\n",
    "           map(parser).\\\n",
    "           batch(batch_size).\\\n",
    "           repeat(10)\n",
    "\n",
    "# establish data_pipeline for testset\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((list(imgs_test),list(labels_test)))\n",
    "dataset_test = dataset_test.\\\n",
    "            map(parser).\\\n",
    "            batch(1)\n",
    "\n",
    "# initialize model function\n",
    "network_train_fn = nets_factory.get_network_fn(\n",
    "    name = model_name,\n",
    "    weight_decay = weight_decay,\n",
    "    num_classes = num_classes,\n",
    "    is_training = True)\n",
    "\n",
    "network_test_fn = nets_factory.get_network_fn(\n",
    "    name = model_name,\n",
    "    weight_decay = weight_decay,\n",
    "    num_classes = num_classes,\n",
    "    is_training = False,\n",
    "    reuse = True)\n",
    "\n",
    "\n",
    "#serial_train(network_train_fn, network_test_fn, dataset_train, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_device(device, ps_device):\n",
    "\n",
    "    def _assign(op):\n",
    "        \n",
    "        PS_OPS = [\n",
    "        'Variable', 'VariableV2', 'AutoReloadVariable', 'MutableHashTable',\n",
    "        'MutableHashTableOfTensors', 'MutableDenseHashTable']\n",
    "        \n",
    "        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n",
    "        \n",
    "        if node_def.op in PS_OPS:\n",
    "            return ps_device\n",
    "        else:\n",
    "            return device\n",
    "    return _assign    \n",
    "\n",
    "def create_parallel_optimization(model_fn, input_fn, optimizer, controller=\"/cpu:0\"):\n",
    "    devices = get_available_gpus()\n",
    "        \n",
    "    tower_grads = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "    \n",
    "    # Get the current variable scope so we can reuse all variables we need once we get\n",
    "    # to the second iteration of the loop below\n",
    "    with tf.variable_scope(tf.get_variable_scope()) as outer_scope:\n",
    "        for i, id in enumerate(devices):\n",
    "            name = 'tower_{}'.format(i)\n",
    "            # Use the assign_to_device function to ensure that variables are created on the\n",
    "            # controller.\n",
    "            with tf.device(assign_to_device(id, controller)), tf.name_scope(name):\n",
    "                \n",
    "                # Compute loss and gradients, but don't apply them yet\n",
    "                loss, logits_train, labels_train = forward_backward(model_fn, input_fn())\n",
    "                equality_train = tf.equal(labels_train, tf.cast(tf.argmax(logits_train, 1), tf.int32))\n",
    "                acc_train = tf.reduce_mean(tf.cast(equality_train, tf.float32))\n",
    "\n",
    "                with tf.name_scope(\"compute_gradients\"):\n",
    "                    # `compute_gradients` returns a list of (gradient, variable) pairs\n",
    "                    grads = optimizer.compute_gradients(loss)\n",
    "\n",
    "                    tower_grads.append(grads)\n",
    "                    \n",
    "                accs.append(acc_train)\n",
    "                losses.append(loss)\n",
    "            # After the first iteration, we want to reuse the variables.\n",
    "            outer_scope.reuse_variables()\n",
    "            \n",
    "    # Apply the gradients on the controlling device\n",
    "    with tf.name_scope(\"apply_gradients\"), tf.device(controller):\n",
    "        # Note that what we are doing here mathematically is equivalent to returning the\n",
    "        # average loss over the towers and compute the gradients relative to that.\n",
    "        # Unfortunately, this would place all gradient-computations on one device, which is\n",
    "        # why we had to compute the gradients above per tower and need to average them here.\n",
    "        \n",
    "        # This function is defined below; it takes the list of (gradient, variable) lists\n",
    "        # and turns it into a single (gradient, variables) list.\n",
    "        gradients = average_gradients(tower_grads)\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        apply_gradient_op = optimizer.apply_gradients(gradients, global_step)\n",
    "        avg_loss = tf.reduce_mean(losses)\n",
    "        avg_acc = tf.reduce_mean(accs)\n",
    "    return apply_gradient_op, avg_loss, avg_acc\n",
    "\n",
    "# Source:\n",
    "# https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "        Returns a list of the identifiers of all visible GPUs.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "\n",
    "# Source:\n",
    "# https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L101\n",
    "def average_gradients(tower_grads):\n",
    "    gradvars = []\n",
    "    with tf.name_scope('gradient_averaging'):\n",
    "        all_grads = {}\n",
    "        for grad, var in itertools.chain(*tower_grads):\n",
    "            if grad is not None:\n",
    "                all_grads.setdefault(var, []).append(grad)\n",
    "        for var, grads in six.iteritems(all_grads):\n",
    "            # Average gradients on the same device as the variables\n",
    "            # to which they apply.\n",
    "            with tf.device(var.device):\n",
    "                if len(grads) == 1:\n",
    "                    avg_grad = grads[0]\n",
    "                else:\n",
    "                    avg_grad = tf.multiply(tf.add_n(grads), 1. / len(grads))\n",
    "            gradvars.append((avg_grad, var))\n",
    "    return gradvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_training(model_train_fn, model_test_fn, input_train_fn, input_test_fn):\n",
    "    \n",
    "    geneartor_train = input_train_fn.make_one_shot_iterator()\n",
    "    genearator_test = input_test_fn.make_one_shot_iterator()\n",
    "    \n",
    "    def input_train_fn():\n",
    "        with tf.device(None):\n",
    "        # remove any device specifications for the input data\n",
    "            return geneartor_train.get_next()\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1E-3)\n",
    "    \n",
    "    update_op, loss, acc_train = create_parallel_optimization(model_train_fn, input_train_fn, optimizer)\n",
    "    \n",
    "    logits_test, labels_test = forward(model_test_fn, genearator_test.get_next())\n",
    "    equality_test = tf.equal(labels_test, tf.cast(tf.argmax(logits_test, 1), tf.int32))\n",
    "    acc_test = tf.reduce_mean(tf.cast(equality_test, tf.float32))\n",
    "\n",
    "    \n",
    "    # list of parameters to load\n",
    "    get_model_var_list = lambda : slim.get_model_variables()\n",
    "    is_in_exclusion = lambda exclusions, x : [x for exclusion in exclusions if x.op.name.startswith(exclusion) ]\n",
    "    is_in_model_exclusion = functools.partial(is_in_exclusion, variable_to_exclude)\n",
    "    variable_to_include = list(set(get_model_var_list()) - set(filter(is_in_model_exclusion, get_model_var_list())))\n",
    "    \n",
    "    weights_loader = slim.assign_from_checkpoint_fn(\n",
    "        pretrained_dir,\n",
    "        variable_to_include,\n",
    "        ignore_missing_vars=True)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        weights_loader(sess)\n",
    "        start_training(sess, loss, update_op, acc_train, acc_test)\n",
    "                \n",
    "parallel_training(network_train_fn, network_test_fn, dataset_train, dataset_test)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu-1.13",
   "language": "python",
   "name": "tensorflow-gpu-1.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
